Stateful Window Operations
==========================
There are two kinds of operations that we can perform on streaming data typically:

1. Stateless operations - are transformations which are applied on a single stream entity, a single record.

2. Stateful operations - consider multiple streaming entities at the same time. These are transformations which accumulate
   across these multiple entities.

A lot of stream processing involves using window operations. Windows are when we want to accumulate information across a particular window of time
in a stream. Window transformations are a way to operate on a subset of the streaming data.
Spark supports window operations based on a certain time interval; that means transformations can be applied on all entities within a time window,
and the kind of transformations can be sum, min, max, average; any aggregation.


Tumbling, Sliding and Global Windows
====================================
Spark supports the tumbling, sliding, and the global window.
Let's assume that we have a stream of data. The different entities in our streaming data come in over a period of time.
That means every entity is at a different timestamp.

Tumbling Window
---------------
The tumbling window is a window which is a fixed interval of time. The window size is fixed. In tumbling windows, time is non-overlapping.
That means a particular streaming entity can belong to just one window. The time interval that defines a tumbling window is fixed,
but the number of entities within a window can differ. So if you have a lot of entities come in at a particular point in time,
such as error messages from your website logs, there will be many entities at a particular fixed interval of time when the error occurs.
The tumbling window is so called because it tumbles over the streaming data in a non-overlapping manner.
No streaming entity is part of more than one window in a tumbling window.

Sliding Window
--------------
The sliding window, once again, has a fixed window size, which is based on the window interval.
Sliding windows differ from tumbling windows in the fact that consecutive windows are overlapping in time.
This overlap period is defined as the sliding interval. Once again, as in the case of tumbling windows, the number of streaming entities
within a window will defer based on how many entities came in in a particular interval of time.
When you use a sliding window, a particular entity can belong to more than one window.
As the window slides over, one or more streaming entities can belong to consecutive windows.
Sliding windows tend to be the most commonly used kind of windows because they are great for showing how trends gradually change over time.

Global Window
-------------
If you consider data accumulated across the stream as a whole from the very beginning of time, that is a global window.


Event, Ingestion, and Processing Time
=====================================
When you're working with streaming data, unlike with batch processing, the notion of time becomes very important
because generally when you perform aggregations on streaming data, you use windows, and windows are generally based on time.
Tumbling, as well as sliding windows, consider all the entities in a fixed interval of time.

There are different notions of time that can apply to entities in a stream.
There is the event time, the time at which the event originally occurred; there is the ingestion time, the time at which the event came to Spark;
and there is the processing time, the time at which the data was processed by Spark.

Event Time
----------
Event time refers to the time at which the event occurred at its original source. The original source could be anything in streaming data.
It could be the mobile phone that sends out GPS coordinates, the sensor which collects climate data, the website which sends out log messages.
Event time is generally embedded as a part of the streaming data. Event time is something that is known at the source, not during Spark processing.
Event time allows us to order the data so that the streaming data appears in the right order.
It allows us to get correct results even if a particular data point appears out of order or late.

Ingestion Time
--------------
Ingestion time is the time at which a particular streaming entity enters the Spark application via some kind of source.
This is a timestamp that may be assigned by the Spark system and is chronologically after the event time,
which means ingestion time cannot be used to order out of order events.


Processing Time
---------------
Processing time refers to when the system actually processes the streaming entities. This is the system time when processing starts.
This is chronologically after the event time and the ingestion time, and is non-deterministic.
It depends on when the data arrives, how long the processing of the data takes, and so on.
Processing time tends to be the simplest time to work with because there is no coordination required between the origin of the streams and
the actual processing.


Watermarks and Late Data
========================
Handling late arriving events is a crucial functionality for Stream Processing Engines. A solution to this problem is the concept of watermarking.
Watermarking is a useful method which helps a Stream Processing Engine to deal with lateness.
Basically, a watermark is a threshold to specify how long the system waits for late events.
If an arriving event lies within the watermark, it gets used to update a query.
Otherwise, if it’s older than the watermark, it will be dropped and not further processed by the Streaming Engine.

In distributed and networked systems, there’s always a chance for disruption — nodes going down, sensors are loosing connection and so on.
Because of that, it’s not guaranteed that data will arrive in a Stream Processing Engine in the order they were created.
For the sake of fault tolerance it’s therefore necessary to handle such Out-of-Order data.

You can enable watermark by simply adding the withWatermark() operator to a query:

"withWatermark(eventTimeColumn: String, delayThreshold: String): Dataset[T]"

It takes two Parameters:
a) an event time column (must be the same as the aggregate is working on.
b) a threshold to specify for how long late data should be processed (in event time unit).

The state of an aggregate will then be maintained by Spark until "(MaxEventTime — delayThreshold) > T" ,
where MaxEventTime is the latest event time seen by the engine and T is the starting time of a window.

If late data fall within this threshold, the query gets updated eventually.
Otherwise it gets dropped and no reprocessing is triggered.

It’s important to mention that the output mode of the query must be set either to "append" (which is the default) or "update”.
Complete-mode can’t be used in conjunction with watermarking by design, because it requires all the data to be preserved
for outputting the whole result table to a sink.